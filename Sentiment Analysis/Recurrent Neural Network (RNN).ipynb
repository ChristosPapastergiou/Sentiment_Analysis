{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66593,"databundleVersionId":7351814,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Includes","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport spacy\nimport torch\nimport pickle\nimport optuna\nimport scikitplot\nimport matplotlib\nimport unicodedata\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')              # Download to be able to use word_tokenize from nltk library\n\n!pip install optuna                 # Download to be able to use Optuna as hyper-parameter\n!python -m spacy download el        # Download to be able to lemmatize greek words using spacy\n!pip install greek-stemmer-pos      # Download to be able to use stem on greek words\n\nfrom tabulate import tabulate\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom greek_stemmer import stemmer\n\nfrom gensim.models import Word2Vec\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T23:58:35.482010Z","iopub.execute_input":"2024-02-06T23:58:35.482606Z","iopub.status.idle":"2024-02-06T23:59:23.871927Z","shell.execute_reply.started":"2024-02-06T23:58:35.482554Z","shell.execute_reply":"2024-02-06T23:59:23.870320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"# ---- Reading the datasets ----\n\ndf_test = pd.read_csv('/kaggle/input/ys19-2023-assignment-3/test_set.csv')\ndf_train = pd.read_csv('/kaggle/input/ys19-2023-assignment-3/train_set.csv')\ndf_valid = pd.read_csv('/kaggle/input/ys19-2023-assignment-3/valid_set.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:59:28.619418Z","iopub.execute_input":"2024-02-06T23:59:28.620533Z","iopub.status.idle":"2024-02-06T23:59:29.108702Z","shell.execute_reply.started":"2024-02-06T23:59:28.620483Z","shell.execute_reply":"2024-02-06T23:59:29.107504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing","metadata":{}},{"cell_type":"code","source":"# ---- NaN values ----\n\ndf_test.dropna(inplace = True)   # Good practice to\ndf_train.dropna(inplace = True)  # remove all empty   \ndf_valid.dropna(inplace = True)  # values of the datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:59:31.006525Z","iopub.execute_input":"2024-02-06T23:59:31.006980Z","iopub.status.idle":"2024-02-06T23:59:31.043987Z","shell.execute_reply.started":"2024-02-06T23:59:31.006946Z","shell.execute_reply":"2024-02-06T23:59:31.042769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Stopwords ----\n\nnltk.download('stopwords')\nnlp = spacy.load('el_core_news_sm')\n\nspacy_greek_stopwords = nlp.Defaults.stop_words       # Greek stopwords from spacy\nnltk_greek_stopwords = set(stopwords.words('greek'))  # Greek stopwords from nltk\n\nenglish_stopwords = set(stopwords.words('english'))\ngreek_stopwords = spacy_greek_stopwords.union(nltk_greek_stopwords) # Combination of those so there is a bigger list of stopwords","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:59:33.011961Z","iopub.execute_input":"2024-02-06T23:59:33.012479Z","iopub.status.idle":"2024-02-06T23:59:34.310671Z","shell.execute_reply.started":"2024-02-06T23:59:33.012437Z","shell.execute_reply":"2024-02-06T23:59:34.309260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Cleansing / Tokenizing ----\n\nmapping_names = {\n    'αλεξη' : 'αλεξης',\n    'τσιπρα' : 'τσιπρας',\n    'συριζας' : 'συριζα',\n    'κουλη' : 'κυριακος',\n    'κουλης' : 'κυριακος',\n    'κυριακο' : 'κυριακος',\n    'μητσοτακη' : 'μητσοτακης',\n}\n\ndef replace_words(text):\n    words = text.split()                                                # If the word is inside the dictionary (map for names)\n    replaced_words = [mapping_names.get(word, word) for word in words]  # change it. Lemmatizer can't convert those names and if \n    return ' '.join(replaced_words)                                     # there's no usage of lemmatizer names must be equal\n\ndef split_connected(text):\n    english_letters = r'[A-Za-z]+'\n    greek_letters = r'[Α-Ωα-ωίϊΐόάέύϋΰήώ]+'\n    \n    greek = re.findall(greek_letters, text)     # Go and find the greek and the english \n    english = re.findall(english_letters, text) # words based on the patern above\n    \n    return ' '.join(english + greek)  # End combine them to have cleaner text. For example \"Greekεκλογες\" will be \"Greek εκλογες\"\n\ndef remove_accent(text):\n    normalized = unicodedata.normalize('NFD', text)                              # Using unicodedata to decompose characters\n    new_text = [char for char in normalized if not unicodedata.combining(char)]  # with tones or any accent and be able to keep\n    return ''.join(new_text)\n\ndef stem_text(text):\n    words = text.split()                                               # Split the text into words\n    stemmed_words = [stemmer.stem_word(word,'VBG') for word in words]  # stem every word of the text \n    return ' '.join(stemmed_words)                                     # and join all text pieces\n\ndef lemmatize_text(text):\n    tokens = nlp(text)                                     # Tokenize the text\n    lemmatized_words = [token.lemma_ for token in tokens]  # then apply lemmatization\n    return ' '.join(lemmatized_words)                      # and join all the tokens\n\ndef text_cleaning(column):\n    column = column.str.lower()                                # All to lowercase first\n    column = column.str.replace(r'[@#]', '', regex = True)     # Remove all mentions and hashtags\n    column = column.str.replace(r'www\\S+', '', regex = True)   # Remove anything that \n    column = column.str.replace(r'http\\S+', '', regex = True)  # has to do with links \n    column = column.str.replace(r'[^\\w\\s]', '', regex = True)  # Remove all punctuation marks\n       \n    column = column.apply(split_connected)\n    \n    column = column.apply(lambda x: ' '.join(x.split())) # Replace consecutive whitespaces so the resulting description will contain a single space between words\n    column = column.apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x)) # Replace 3 or more same consecutive characters by 2 using regular expressions (e.g balll will become ball)\n    column = column.apply(lambda x: ' '.join([word for word in x.split() if word not in greek_stopwords]))   # Remove greek stopwords\n    column = column.apply(lambda x: ' '.join([word for word in x.split() if word not in english_stopwords])) # Remove english stopwords\n\n#     column = column.apply(stem_text)\n#     column = column.str.lower()\n#     column = column.apply(lemmatize_text)\n    column = column.apply(remove_accent)\n    column = column.apply(replace_words)\n    \n    return column\n\ndf_test['Text'] = text_cleaning(df_test['Text'])\ndf_train['Text'] = text_cleaning(df_train['Text'])\ndf_valid['Text'] = text_cleaning(df_valid['Text'])\n\ndf_test['Tokens'] = df_test['Text'].apply(word_tokenize)\ndf_train['Tokens'] = df_train['Text'].apply(word_tokenize)\ndf_valid['Tokens'] = df_valid['Text'].apply(word_tokenize)\n\ndf_train['Sentiment_encoded'] = LabelEncoder().fit_transform(df_train['Sentiment'])\ndf_valid['Sentiment_encoded'] = LabelEncoder().fit_transform(df_valid['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:59:37.446723Z","iopub.execute_input":"2024-02-06T23:59:37.447350Z","iopub.status.idle":"2024-02-06T23:59:56.413902Z","shell.execute_reply.started":"2024-02-06T23:59:37.447305Z","shell.execute_reply":"2024-02-06T23:59:56.412566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"# ---- Pie Plots ----\n\nplt.figure(figsize = (20, 8))\n\n# What is the distribution of the sentiment\ntotal_sentiment_distribution = df_train['Sentiment'].value_counts()\n\nplt.subplot(1, 3, 1)\ntotal_sentiment_distribution.plot(kind = 'pie', autopct = '%1.1f%%')\nplt.title('Sentiments')\n\n# Whats the total votes every party has\ntotal_votes_party = df_train['Party'].value_counts()\n\nplt.subplot(1, 3, 2)\ntotal_votes_party.plot(kind = 'pie', autopct = lambda p: '{:.0f}'.format(p * total_votes_party.sum() / 100))\nplt.title('Party Votes')\n\n# How many positive sentiments each party has\npositive_sentiment_party = df_train[df_train['Sentiment'] == 'POSITIVE']['Party'].value_counts()\n\nplt.subplot(1, 3, 3)\npositive_sentiment_party.plot(kind = 'pie', autopct = lambda p: '{:.0f}'.format(p * positive_sentiment_party.sum() / 100))\nplt.title('Positive Sentiments by Party')\n\n# Show all analysis #\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:00:18.920614Z","iopub.execute_input":"2024-02-07T00:00:18.921139Z","iopub.status.idle":"2024-02-07T00:00:19.493821Z","shell.execute_reply.started":"2024-02-07T00:00:18.921084Z","shell.execute_reply":"2024-02-07T00:00:19.492731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Bar Plots ----\n\nplt.figure(figsize = (20, 8))\n\ndef find_top_words(column):\n    count = Counter(word for words in column for word in words)      # Calculate word frequencies using Counter\n    top_words = count.most_common(15)                                # and select the top 10 of them to show\n    return pd.DataFrame(top_words, columns = ['Word', 'Frequency'])  # Create a DataFrame for plotting\n\n# Top words used on Tweeter for the Greek Elections \ntop_tweet_words = find_top_words(df_train['Tokens'])\n\nplt.bar(top_tweet_words['Word'], top_tweet_words['Frequency'])\nplt.xticks(rotation = 40)\nplt.title('Top words Tweeted')\nplt.xlabel('Word')\nplt.ylabel('Frequency')\n\n# Show all analysis #\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:00:25.731352Z","iopub.execute_input":"2024-02-07T00:00:25.731833Z","iopub.status.idle":"2024-02-07T00:00:26.369453Z","shell.execute_reply.started":"2024-02-07T00:00:25.731794Z","shell.execute_reply":"2024-02-07T00:00:26.368091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for all dataset's text ----\n\nplt.figure(figsize = (20, 8))\n\ncloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train['Text']))\n\nplt.imshow(cloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('WordCloud')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:00:32.574541Z","iopub.execute_input":"2024-02-07T00:00:32.575034Z","iopub.status.idle":"2024-02-07T00:00:40.494410Z","shell.execute_reply.started":"2024-02-07T00:00:32.574996Z","shell.execute_reply":"2024-02-07T00:00:40.493216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for each Party ----\n\nplt.figure(figsize = (20, 5))\n\nparties = df_train['Party'].unique()\n\nfig, axes = plt.subplots(nrows = (len(parties) // 3) + (len(parties) % 3 > 0), ncols = 3, figsize = (18, 6)) # Many word clouds so split for plotting\n\nfor i, party in enumerate(parties, start = 1):\n    cloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train[df_train['Party'] == party]['Text']))\n    \n    if len(parties) > 3:    # In a dataset im not\n        row = (i - 1) // 3  # sure how many parties there \n        col = (i - 1) % 3   # are maybe it will come in handy\n        ax = axes[row, col]\n    else:\n        ax = axes[i - 1]\n    \n    ax.imshow(cloud, interpolation = 'bilinear')\n    ax.axis('off')\n    ax.set_title(f'{party} Word Cloud')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:00:45.437376Z","iopub.execute_input":"2024-02-07T00:00:45.437931Z","iopub.status.idle":"2024-02-07T00:00:58.770951Z","shell.execute_reply.started":"2024-02-07T00:00:45.437886Z","shell.execute_reply":"2024-02-07T00:00:58.769248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for each Sentiment ----\n\nplt.figure(figsize = (20, 5))\n\nsentiments = df_train['Sentiment'].unique()\n\nfig, axes = plt.subplots(nrows = (len(sentiments) // 3) + (len(sentiments) % 3 > 0), ncols = 3, figsize = (18, 6))\n\nfor i, sentiment in enumerate(sentiments, start = 1):\n    cloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train[df_train['Sentiment'] == sentiment]['Text']))\n    \n    if len(sentiments) > 3: # In a dataset im not\n        row = (i - 1) // 3  # sure how many sentiments there\n        col = (i - 1) % 3   # are maybe it will come in handy\n        ax = axes[row, col]\n    else:\n        ax = axes[i - 1]\n    \n    ax.imshow(cloud, interpolation = 'bilinear')\n    ax.axis('off')\n    ax.set_title(f'{sentiment} Word Cloud')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:01:02.890143Z","iopub.execute_input":"2024-02-07T00:01:02.890656Z","iopub.status.idle":"2024-02-07T00:01:13.125945Z","shell.execute_reply.started":"2024-02-07T00:01:02.890614Z","shell.execute_reply":"2024-02-07T00:01:13.124677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Word Cloud based on parties ----\n\nplt.figure(figsize = (20, 8))\n\n# WordCloud for the top 3 Political Parties\ncolumn = df_train[df_train['Party'].isin(['SYRIZA', 'ND', 'PASOK'])]['Text']\ncloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(column))\n\nplt.imshow(cloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 3 Parties Word Cloud')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:01:16.623270Z","iopub.execute_input":"2024-02-07T00:01:16.623745Z","iopub.status.idle":"2024-02-07T00:01:23.440364Z","shell.execute_reply.started":"2024-02-07T00:01:16.623701Z","shell.execute_reply":"2024-02-07T00:01:23.439368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization, Word Embeddings & Data partioning","metadata":{}},{"cell_type":"code","source":"# ---- Embeddings, Train / Valid (test) ----\n\ndef word_embeddings(column, model):\n    embedding_vector = np.zeros((len(column), model.vector_size)) # Initialization of embeddings with zeros\n    word_count = [len(sublist) for sublist in column]             # Count the words of every text\n\n    for i, sentence in enumerate(column):\n        embeddings = [model.wv[word] for word in sentence if word in model.wv] # .wv is vector that describes the word in numbers\n        if word_count[i] > 0:\n            embedding_vector[i] = sum(embeddings) / word_count[i]\n            \n    return embedding_vector\n\nvector = Word2Vec(df_train['Tokens'], vector_size = 200, seed = 42, epochs = 20)\nvector.train(df_train['Tokens'], total_examples = len(df_train['Tokens']), epochs = vector.epochs)\n\nX_train = word_embeddings(df_train['Tokens'], vector)\nY_train = df_train['Sentiment_encoded']\n        \nX_valid = word_embeddings(df_valid['Tokens'], vector)\nY_valid = df_valid['Sentiment_encoded']","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:11:25.315951Z","iopub.execute_input":"2024-02-07T00:11:25.317456Z","iopub.status.idle":"2024-02-07T00:11:54.253673Z","shell.execute_reply.started":"2024-02-07T00:11:25.317409Z","shell.execute_reply":"2024-02-07T00:11:54.252106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensors","metadata":{}},{"cell_type":"code","source":"# ---- Saving to tensors ----\n\nx_train = torch.tensor(X_train, dtype = torch.float).view(-1, 1, vector.vector_size) \ny_train = torch.tensor(Y_train.values, dtype = torch.long)\nx_valid = torch.tensor(X_valid, dtype = torch.float).view(-1, 1, vector.vector_size)\ny_valid = torch.tensor(Y_valid.values, dtype = torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:12:23.189506Z","iopub.execute_input":"2024-02-07T00:12:23.189994Z","iopub.status.idle":"2024-02-07T00:12:23.221579Z","shell.execute_reply.started":"2024-02-07T00:12:23.189959Z","shell.execute_reply":"2024-02-07T00:12:23.220170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"# A class for our bidirectional RNN \nclass RNN(nn.Module):\n    def __init__(self, cell_type, input_size, hidden_size, num_layers, dropout, output_size):\n        super(RNN, self).__init__()\n        \n        cells = {\n          'LSTM' : nn.LSTM,\n          'GRU'  : nn.GRU\n        }\n        \n        torch.manual_seed(42)\n        self.activation = nn.Sigmoid()\n        self.linear = nn.Linear(hidden_size * 2, output_size)   # Doubling the hidden size because rnn is bidirectional \n        self.rnn = cells[cell_type](input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout,  \n                                    bidirectional = True, batch_first = True)\n\n    def forward(self, x):\n        rnn_out, _ = self.rnn(x)\n        out = rnn_out[:, -1, :]\n        out = self.activation(self.linear(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:12:25.487643Z","iopub.execute_input":"2024-02-07T00:12:25.488505Z","iopub.status.idle":"2024-02-07T00:12:25.499254Z","shell.execute_reply.started":"2024-02-07T00:12:25.488449Z","shell.execute_reply":"2024-02-07T00:12:25.497924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Setting up criterion, optimizer, data ----\n\nmodel = RNN('GRU', x_train.shape[2], 64, 2, 0.2, 3)\n\ncriterion = nn.CrossEntropyLoss()                           # Initialize criterion\noptimizer = torch.optim.Adam(model.parameters(), lr = 5e-5) # Initialize optimizer\n\ndataset = TensorDataset(x_train, y_train)                          # Class to represent the data as list of tensors\ndataloader = DataLoader(dataset, batch_size = 64, shuffle = True)  # Initialize dataloader","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:22:24.806792Z","iopub.execute_input":"2024-02-07T00:22:24.807277Z","iopub.status.idle":"2024-02-07T00:22:24.823338Z","shell.execute_reply.started":"2024-02-07T00:22:24.807238Z","shell.execute_reply":"2024-02-07T00:22:24.822144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Training","metadata":{}},{"cell_type":"code","source":"for epoch in range(10):\n    losses = []\n    model.train()\n    for x, y in dataloader:\n        optimizer.zero_grad()       # Delete previously stored gradients\n        \n        logits = model(x)           # Perform forward propagation\n        loss = criterion(logits, y) # Calculating the loss using criterion function\n        losses.append(loss.item())\n                       \n        loss.backward()                                    # Perform back propagation starting from the loss calculated in this epoch\n        nn.utils.clip_grad_norm_(model.parameters(), 1e-3) # Gradient clipping to prevent exploding gradients\n        optimizer.step()                                   # Update model's weights based on the gradients calculated during backprop\n        \n    print(f'Epoch {epoch}: Loss = {sum(losses) / len(dataloader):.5f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:22:26.536703Z","iopub.execute_input":"2024-02-07T00:22:26.537395Z","iopub.status.idle":"2024-02-07T00:23:14.035052Z","shell.execute_reply.started":"2024-02-07T00:22:26.537340Z","shell.execute_reply":"2024-02-07T00:23:14.034090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    logits_valid = model(x_valid)\n    probabilities = nn.functional.softmax(logits_valid, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\naccuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\nf1 = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)                 \nrecall = recall_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)        \nprecision = precision_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n                          \ntable = [['Method', 'Accuracy', 'F1-Score', 'Recall', 'Precision'],\n         ['Predict', accuracy, f1, recall, precision]]\n\nprint(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:23:15.372973Z","iopub.execute_input":"2024-02-07T00:23:15.373749Z","iopub.status.idle":"2024-02-07T00:23:15.429919Z","shell.execute_reply.started":"2024-02-07T00:23:15.373690Z","shell.execute_reply":"2024-02-07T00:23:15.428682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    num_layers = trial.suggest_int('num_layers', 2, 3) \n    dropout = trial.suggest_float('dropout', 0.0, 0.5)  \n    hidden_size =  trial.suggest_categorical('hidden_size', [64, 128])\n    cell_type = trial.suggest_categorical('cell_type', ['LSTM', 'GRU'])\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log = True)\n    \n    model = RNN(cell_type, x_train.shape[2], hidden_size, num_layers, dropout, 3)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n    dataset = TensorDataset(x_train, y_train)\n    dataloader = DataLoader(dataset, batch_size = 64, shuffle = True) \n    \n    for epoch in range(10):\n        model.train()\n        for x, y in dataloader:\n            optimizer.zero_grad()       # Delete previously stored gradients\n            \n            logits = model(x)           # Perform forward propagation\n            loss = criterion(logits, y) # Calculating the loss using criterion function\n            \n            loss.backward()                                    # Perform back propagation starting from the loss calculated in this epoch\n            nn.utils.clip_grad_norm_(model.parameters(), 1e-3) # Gradient clipping to prevent exploding gradients\n            optimizer.step()                                   # Update model's weights based on the gradients calculated during backprop\n                    \n    model.eval()\n    with torch.no_grad():\n        logits_valid = model(x_valid)\n        probabilities = nn.functional.softmax(logits_valid, dim = 1)\n        predictions = torch.argmax(probabilities, dim = 1)\n\n    accuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\n        \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize', study_name = 'Christos study')\nstudy.optimize(objective, n_trials = 100)\nbest_parameters = study.best_params\n\nbest_dropout = best_parameters['dropout']\nbest_lr = best_parameters['learning_rate']\nbest_cell_type = best_parameters['cell_type']\nbest_num_layers = best_parameters['num_layers']\nbest_hidden_size = best_parameters['hidden_size']\n\nprint('Best hyperparameters: ', best_parameters)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T00:23:41.785782Z","iopub.execute_input":"2024-02-07T00:23:41.786361Z","iopub.status.idle":"2024-02-07T02:21:55.201311Z","shell.execute_reply.started":"2024-02-07T00:23:41.786315Z","shell.execute_reply":"2024-02-07T02:21:55.199865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network using Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"# ---- Setting up criterion, optimizer, data ----\n\nmodel = RNN(best_cell_type, x_train.shape[2], best_hidden_size, best_num_layers, best_dropout, 3)\n\ncriterion = nn.CrossEntropyLoss()                              # Initialize criterion\noptimizer = torch.optim.Adam(model.parameters(), lr = best_lr) # Initialize optimizer\n\ndataset = TensorDataset(x_train, y_train)                           # Class to represent the data as list of tensors\ndataloader = DataLoader(dataset, batch_size = 64, shuffle = True)   # Initialize dataloader","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:22:08.613710Z","iopub.execute_input":"2024-02-07T02:22:08.614223Z","iopub.status.idle":"2024-02-07T02:22:08.636175Z","shell.execute_reply.started":"2024-02-07T02:22:08.614172Z","shell.execute_reply":"2024-02-07T02:22:08.635085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training using Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"for epoch in range(10):\n    losses = []\n    model.train()\n    for x, y in dataloader:\n        optimizer.zero_grad()       # Delete previously stored gradients\n        \n        logits = model(x)           # Perform forward propagation\n        loss = criterion(logits, y) # Calculating the loss using criterion function\n        losses.append(loss.item())\n        \n        loss.backward()                                    # Perform back propagation starting from the loss calculated in this epoch\n        nn.utils.clip_grad_norm_(model.parameters(), 1e-3) # Gradient clipping to prevent exploding gradients\n        optimizer.step()                                   # Update model's weights based on the gradients calculated during backprop\n        \n    print(f'Epoch {epoch}: Loss = {sum(losses) / len(dataloader):.5f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:22:10.759108Z","iopub.execute_input":"2024-02-07T02:22:10.759571Z","iopub.status.idle":"2024-02-07T02:23:49.715164Z","shell.execute_reply.started":"2024-02-07T02:22:10.759535Z","shell.execute_reply":"2024-02-07T02:23:49.713840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation with Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    logits_valid = model(x_valid)\n    probabilities = nn.functional.softmax(logits_valid, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\naccuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\nf1 = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)                 \nrecall = recall_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)        \nprecision = precision_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n                          \ntable = [['Method', 'Accuracy', 'F1-Score', 'Recall', 'Precision'],\n         ['Predict', accuracy, f1, recall, precision]]\n\nprint(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:23:51.429749Z","iopub.execute_input":"2024-02-07T02:23:51.430933Z","iopub.status.idle":"2024-02-07T02:23:51.570107Z","shell.execute_reply.started":"2024-02-07T02:23:51.430891Z","shell.execute_reply":"2024-02-07T02:23:51.569144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Curve, ROC, Confusion Matrix","metadata":{}},{"cell_type":"code","source":"score_train = []\nscore_valid = []\nlosses_train = []\nlosses_valid = []\n    \nfor i in range(2):\n    model = RNN('GRU', x_train.shape[2], 64, 2, 0.2, 3)\n    # model = RNN(best_cell_type, x_train.shape[2], best_hidden_size, best_num_layers, best_dropout, 3)\n\n    criterion = nn.CrossEntropyLoss()                               # Initialize criterion\n    optimizer = torch.optim.Adam(model.parameters(), lr = 5e-5)     # Initialize optimizer\n\n    dataset_train = TensorDataset(x_train, y_train)                               # Class to represent the data as list of tensors\n    dataloader_train = DataLoader(dataset_train, batch_size = 64, shuffle = True) # Initialize dataloader\n\n    dataset_valid = TensorDataset(x_valid, y_valid)                               # Class to represent the data as list of tensors\n    dataloader_valid = DataLoader(dataset_valid, batch_size = 64, shuffle = True) # Initialize dataloader\n\n    if i == 0:\n        for epoch in range(10):\n            model.train()\n            for x, y in dataloader_train:\n                optimizer.zero_grad()       # Delete previously stored gradients\n                \n                logits = model(x)           # Perform forward propagation\n                loss = criterion(logits, y) # Calculating the loss using criterion function\n\n                loss.backward()                                    # Perform back propagation starting from the loss calculated in this epoch\n                nn.utils.clip_grad_norm_(model.parameters(), 1e-3) # Gradient clipping to prevent exploding gradients\n                optimizer.step()                                   # Update model's weights based on the gradients calculated during backprop\n\n            model.eval()\n            with torch.no_grad():\n                logits_train = model(x_train)\n                probabilities = nn.functional.softmax(logits_train, dim = 1)\n                predictions = torch.argmax(probabilities, dim = 1)\n\n            f1_train = f1_score(y_train.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n            score_train.append(f1_train)\n\n            model.eval()\n            with torch.no_grad():\n                logits_valid = model(x_valid)\n                probabilities = nn.functional.softmax(logits_valid, dim = 1)\n                predictions = torch.argmax(probabilities, dim = 1)\n\n            f1_valid = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n            score_valid.append(f1_valid)\n        \n    else:\n        for epoch in range(10):\n            losses = []\n            model.train()\n            for x, y in dataloader_train:\n                optimizer.zero_grad()       # Delete previously stored gradients\n                \n                logits = model(x)           # Perform forward propagation\n                loss = criterion(logits, y) # Calculating the loss using criterion function\n                losses.append(loss.item())\n\n                loss.backward()                                    # Perform back propagation starting from the loss calculated in this epoch\n                nn.utils.clip_grad_norm_(model.parameters(), 1e-3) # Gradient clipping to prevent exploding gradients\n                optimizer.step()                                   # Update model's weights based on the gradients calculated during backprop\n\n            loss = sum(losses) / len(dataloader_train)\n            losses_train.append(loss)\n\n            losses = []\n            model.eval()\n            with torch.no_grad():\n                for x, y in dataloader_valid:\n                    logits = model(x)           # Perform forward propagation\n                    loss = criterion(logits, y) # Calculating the loss using criterion function\n                    losses.append(loss.item())\n\n                loss = sum(losses) / len(dataloader_valid)\n                losses_valid.append(loss)\n\nplt.figure(figsize = (20, 8))\n\nplt.subplot(1, 2, 1)\nplt.plot(score_train, label = 'Training')\nplt.plot(score_valid, label = 'Validation')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(losses_train, label = 'Training')\nplt.plot(losses_valid, label = 'Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:24:21.328432Z","iopub.execute_input":"2024-02-07T02:24:21.328887Z","iopub.status.idle":"2024-02-07T02:26:02.646098Z","shell.execute_reply.started":"2024-02-07T02:24:21.328853Z","shell.execute_reply":"2024-02-07T02:26:02.645175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- ROC Curve / AUC Score ----\n\nauc = np.round(roc_auc_score(y_valid.numpy(), probabilities.detach().numpy(), multi_class = 'ovo'), 3)\nprint('Total AUC: ', auc)\n\nscikitplot.metrics.plot_roc(y_valid.numpy(), probabilities.detach().numpy())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:26:06.642092Z","iopub.execute_input":"2024-02-07T02:26:06.642599Z","iopub.status.idle":"2024-02-07T02:26:07.057049Z","shell.execute_reply.started":"2024-02-07T02:26:06.642558Z","shell.execute_reply":"2024-02-07T02:26:07.056191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Confusion Matrix ----\n\nconfusion_martix = metrics.confusion_matrix(y_valid.numpy(), predictions.numpy())\n\nsns.heatmap(confusion_martix, annot = True, fmt = 'd', cmap = 'coolwarm')\nplt.ylabel('Prediction', fontsize = 13)\nplt.xlabel('Actual', fontsize = 13)\nplt.title('Confusion Matrix', fontsize = 17)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:26:09.928481Z","iopub.execute_input":"2024-02-07T02:26:09.928935Z","iopub.status.idle":"2024-02-07T02:26:10.304642Z","shell.execute_reply.started":"2024-02-07T02:26:09.928900Z","shell.execute_reply":"2024-02-07T02:26:10.303435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Predictions","metadata":{}},{"cell_type":"code","source":"def replace_sentiment(column):\n    map_sentiments = {\n        0: 'NEGATIVE',\n        1: 'NEUTRAL',\n        2: 'POSITIVE',\n    }\n    return list(map(map_sentiments.get, column))\n\nX_test = word_embeddings(df_test['Tokens'], vector)\nx_test = torch.tensor(X_test, dtype = torch.float).view(-1, 1, vector.vector_size)\n\nmodel.eval()\nwith torch.no_grad():\n    logits_test = model(x_test)\n    probabilities = nn.functional.softmax(logits_test, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\ndf_test['Sentiment'] = predictions\ndf_test['Sentiment'] = replace_sentiment(df_test['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:26:13.328021Z","iopub.execute_input":"2024-02-07T02:26:13.328543Z","iopub.status.idle":"2024-02-07T02:26:14.105870Z","shell.execute_reply.started":"2024-02-07T02:26:13.328505Z","shell.execute_reply":"2024-02-07T02:26:14.104551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = df_test[['New_ID', 'Sentiment']]                                            # The dataset to output needs only\ndf_predictions = df_predictions.rename(columns = {'New_ID': 'Id', 'Sentiment': 'Predicted'}) # the 2 columns of sentiments and ids\ndf_predictions.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T02:26:17.605896Z","iopub.execute_input":"2024-02-07T02:26:17.606481Z","iopub.status.idle":"2024-02-07T02:26:17.650523Z","shell.execute_reply.started":"2024-02-07T02:26:17.606436Z","shell.execute_reply":"2024-02-07T02:26:17.649194Z"},"trusted":true},"execution_count":null,"outputs":[]}]}