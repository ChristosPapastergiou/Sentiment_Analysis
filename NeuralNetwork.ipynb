{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64789,"databundleVersionId":7104041,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ---- Includes ----\n\nimport re\nimport nltk\nimport spacy\nimport torch\nimport pickle\nimport optuna\nimport scikitplot\nimport matplotlib\nimport unicodedata\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')              # Download to be able to use word_tokenize from nltk library\n\n!pip install optuna                 # Download to be able to use Optuna as hyper-parameter\n!python -m spacy download el        # Download to be able to lemmatize greek words using spacy\n!pip install greek-stemmer-pos      # Download to be able to use stem on greek words\n!pip install --upgrade scikit-learn # Download to be able to import roc_auc_score (new edition stuff)\n\nfrom tabulate import tabulate\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom greek_stemmer import stemmer\n\nfrom gensim.models import Word2Vec\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:24:54.353860Z","iopub.execute_input":"2023-12-24T21:24:54.354394Z","iopub.status.idle":"2023-12-24T21:26:11.597060Z","shell.execute_reply.started":"2023-12-24T21:24:54.354361Z","shell.execute_reply":"2023-12-24T21:26:11.596171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"# ---- Reading the datasets ----\n\ndf_test = pd.read_csv('/kaggle/input/ys19-2023-assignment-2/test_set.csv')\ndf_train = pd.read_csv('/kaggle/input/ys19-2023-assignment-2/train_set.csv')\ndf_valid = pd.read_csv('/kaggle/input/ys19-2023-assignment-2/valid_set.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:27:01.204320Z","iopub.execute_input":"2023-12-24T21:27:01.205246Z","iopub.status.idle":"2023-12-24T21:27:01.856498Z","shell.execute_reply.started":"2023-12-24T21:27:01.205196Z","shell.execute_reply":"2023-12-24T21:27:01.855441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing","metadata":{}},{"cell_type":"code","source":"# ---- NaN values ----\n\ndf_test.dropna(inplace = True)   # Good practice to\ndf_train.dropna(inplace = True)  # remove all empty   \ndf_valid.dropna(inplace = True)  # values of the datasets","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:27:05.895019Z","iopub.execute_input":"2023-12-24T21:27:05.895436Z","iopub.status.idle":"2023-12-24T21:27:05.918246Z","shell.execute_reply.started":"2023-12-24T21:27:05.895402Z","shell.execute_reply":"2023-12-24T21:27:05.917325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Stopwords ----\n\nnltk.download('stopwords')\nnlp = spacy.load('el_core_news_sm')\n\nspacy_greek_stopwords = nlp.Defaults.stop_words       # Greek stopwords from spacy\nnltk_greek_stopwords = set(stopwords.words('greek'))  # Greek stopwords from nltk\n\nenglish_stopwords = set(stopwords.words('english'))\ngreek_stopwords = spacy_greek_stopwords.union(nltk_greek_stopwords) # Combination of those so there is a bigger list of stopwords","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:27:08.948950Z","iopub.execute_input":"2023-12-24T21:27:08.949553Z","iopub.status.idle":"2023-12-24T21:27:09.881471Z","shell.execute_reply.started":"2023-12-24T21:27:08.949515Z","shell.execute_reply":"2023-12-24T21:27:09.880362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Cleansing / Tokenizing ----\n\nmapping_names = {\n    'αλεξη' : 'αλεξης',\n    'τσιπρα' : 'τσιπρας',\n    'συριζας' : 'συριζα',\n    'κουλη' : 'κυριακος',\n    'κουλης' : 'κυριακος',\n    'κυριακο' : 'κυριακος',\n    'μητσοτακη' : 'μητσοτακης',\n}\n\ndef replace_words(text):\n    words = text.split()                                                # If the word is inside the dictionary (map for names)\n    replaced_words = [mapping_names.get(word, word) for word in words]  # change it. Lemmatizer can't convert those names and if \n    return ' '.join(replaced_words)                                     # there's no usage of lemmatizer names must be equal\n\ndef split_connected(text):\n    english_letters = r'[A-Za-z]+'\n    greek_letters = r'[Α-Ωα-ωίϊΐόάέύϋΰήώ]+'\n    \n    greek = re.findall(greek_letters, text)     # Go and find the greek and the english \n    english = re.findall(english_letters, text) # words based on the patern above\n    \n    return ' '.join(english + greek)  # End combine them to have cleaner text. For example \"Greekεκλογες\" will be \"Greek εκλογες\"\n\ndef remove_accent(text):\n    normalized = unicodedata.normalize('NFD', text)                              # Using unicodedata to decompose characters\n    new_text = [char for char in normalized if not unicodedata.combining(char)]  # with tones or any accent and be able to keep\n    return ''.join(new_text)\n\ndef stem_text(text):\n    words = text.split()                                               # Split the text into words\n    stemmed_words = [stemmer.stem_word(word,'VBG') for word in words]  # stem every word of the text \n    return ' '.join(stemmed_words)                                     # and join all text pieces\n\ndef lemmatize_text(text):\n    tokens = nlp(text)                                     # Tokenize the text\n    lemmatized_words = [token.lemma_ for token in tokens]  # then apply lemmatization\n    return ' '.join(lemmatized_words)                      # and join all the tokens\n\ndef text_cleaning(column):\n    column = column.str.lower()                                # All to lowercase first\n    column = column.str.replace(r'[@#]', '', regex = True)     # Remove all mentions and hashtags\n    column = column.str.replace(r'www\\S+', '', regex = True)   # Remove anything that \n    column = column.str.replace(r'http\\S+', '', regex = True)  # has to do with links \n    column = column.str.replace(r'[^\\w\\s]', '', regex = True)  # Remove all punctuation marks\n       \n    column = column.apply(split_connected)\n    \n    column = column.apply(lambda x: ' '.join(x.split())) # Replace consecutive whitespaces so the resulting description will contain a single space between words\n    column = column.apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x)) # Replace 3 or more same consecutive characters by 2 using regular expressions (e.g balll will become ball)\n    column = column.apply(lambda x: ' '.join([word for word in x.split() if word not in greek_stopwords]))   # Remove greek stopwords\n    column = column.apply(lambda x: ' '.join([word for word in x.split() if word not in english_stopwords])) # Remove english stopwords\n\n#     column = column.apply(stem_text)\n#     column = column.str.lower()\n#     column = column.apply(lemmatize_text)\n    column = column.apply(remove_accent)\n    column = column.apply(replace_words)\n    \n    return column\n\ndf_test['Text'] = text_cleaning(df_test['Text'])\ndf_train['Text'] = text_cleaning(df_train['Text'])\ndf_valid['Text'] = text_cleaning(df_valid['Text'])\n\ndf_test['Tokens'] = df_test['Text'].apply(word_tokenize)\ndf_train['Tokens'] = df_train['Text'].apply(word_tokenize)\ndf_valid['Tokens'] = df_valid['Text'].apply(word_tokenize)\n\ndf_train['Sentiment_encoded'] = LabelEncoder().fit_transform(df_train['Sentiment'])\ndf_valid['Sentiment_encoded'] = LabelEncoder().fit_transform(df_valid['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:27:12.882992Z","iopub.execute_input":"2023-12-24T21:27:12.883394Z","iopub.status.idle":"2023-12-24T21:27:24.317637Z","shell.execute_reply.started":"2023-12-24T21:27:12.883359Z","shell.execute_reply":"2023-12-24T21:27:24.316602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"# ---- Pie Plots ----\n\nplt.figure(figsize = (20, 8))\n\n# What is the distribution of the sentiment\ntotal_sentiment_distribution = df_train['Sentiment'].value_counts()\n\nplt.subplot(1, 3, 1)\ntotal_sentiment_distribution.plot(kind = 'pie', autopct = '%1.1f%%')\nplt.title('Sentiments')\n\n# Whats the total votes every party has\ntotal_votes_party = df_train['Party'].value_counts()\n\nplt.subplot(1, 3, 2)\ntotal_votes_party.plot(kind = 'pie', autopct = lambda p: '{:.0f}'.format(p * total_votes_party.sum() / 100))\nplt.title('Party Votes')\n\n# How many positive sentiments each party has\npositive_sentiment_party = df_train[df_train['Sentiment'] == 'POSITIVE']['Party'].value_counts()\n\nplt.subplot(1, 3, 3)\npositive_sentiment_party.plot(kind = 'pie', autopct = lambda p: '{:.0f}'.format(p * positive_sentiment_party.sum() / 100))\nplt.title('Positive Sentiments by Party')\n\n# Show all analysis #\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:27:29.657098Z","iopub.execute_input":"2023-12-24T21:27:29.657593Z","iopub.status.idle":"2023-12-24T21:27:30.232630Z","shell.execute_reply.started":"2023-12-24T21:27:29.657549Z","shell.execute_reply":"2023-12-24T21:27:30.231629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Bar Plots ----\n\nplt.figure(figsize = (20, 8))\n\ndef find_top_words(column):\n    count = Counter(word for words in column for word in words)      # Calculate word frequencies using Counter\n    top_words = count.most_common(15)                                # and select the top 10 of them to show\n    return pd.DataFrame(top_words, columns = ['Word', 'Frequency'])  # Create a DataFrame for plotting\n\n# Top words used on Tweeter for the Greek Elections \ntop_tweet_words = find_top_words(df_train['Tokens'])\n\nplt.bar(top_tweet_words['Word'], top_tweet_words['Frequency'])\nplt.xticks(rotation = 40)\nplt.title('Top words Tweeted')\nplt.xlabel('Word')\nplt.ylabel('Frequency')\n\n# Show all analysis #\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:29:00.210516Z","iopub.execute_input":"2023-12-24T21:29:00.210899Z","iopub.status.idle":"2023-12-24T21:29:00.714098Z","shell.execute_reply.started":"2023-12-24T21:29:00.210871Z","shell.execute_reply":"2023-12-24T21:29:00.713139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for all dataset's text ----\n\nplt.figure(figsize = (20, 8))\n\ncloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train['Text']))\n\nplt.imshow(cloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('WordCloud')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:30:26.738657Z","iopub.execute_input":"2023-12-24T21:30:26.739065Z","iopub.status.idle":"2023-12-24T21:30:32.589130Z","shell.execute_reply.started":"2023-12-24T21:30:26.739033Z","shell.execute_reply":"2023-12-24T21:30:32.588121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for each Party ----\n\nplt.figure(figsize = (20, 5))\n\nparties = df_train['Party'].unique()\n\nfig, axes = plt.subplots(nrows = (len(parties) // 3) + (len(parties) % 3 > 0), ncols = 3, figsize = (18, 6)) # Many word clouds so split for plotting\n\nfor i, party in enumerate(parties, start = 1):\n    cloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train[df_train['Party'] == party]['Text']))\n    \n    if len(parties) > 3:    # In a dataset im not\n        row = (i - 1) // 3  # sure how many parties there \n        col = (i - 1) % 3   # are maybe it will come in handy\n        ax = axes[row, col]\n    else:\n        ax = axes[i - 1]\n    \n    ax.imshow(cloud, interpolation = 'bilinear')\n    ax.axis('off')\n    ax.set_title(f'{party} Word Cloud')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:30:44.144268Z","iopub.execute_input":"2023-12-24T21:30:44.145128Z","iopub.status.idle":"2023-12-24T21:30:54.656437Z","shell.execute_reply.started":"2023-12-24T21:30:44.145094Z","shell.execute_reply":"2023-12-24T21:30:54.655458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- WordCloud for each Sentiment ----\n\nplt.figure(figsize = (20, 5))\n\nsentiments = df_train['Sentiment'].unique()\n\nfig, axes = plt.subplots(nrows = (len(sentiments) // 3) + (len(sentiments) % 3 > 0), ncols = 3, figsize = (18, 6))\n\nfor i, sentiment in enumerate(sentiments, start = 1):\n    cloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(df_train[df_train['Sentiment'] == sentiment]['Text']))\n    \n    if len(sentiments) > 3: # In a dataset im not\n        row = (i - 1) // 3  # sure how many sentiments there\n        col = (i - 1) % 3   # are maybe it will come in handy\n        ax = axes[row, col]\n    else:\n        ax = axes[i - 1]\n    \n    ax.imshow(cloud, interpolation = 'bilinear')\n    ax.axis('off')\n    ax.set_title(f'{sentiment} Word Cloud')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:31:41.551168Z","iopub.execute_input":"2023-12-24T21:31:41.551570Z","iopub.status.idle":"2023-12-24T21:31:49.323547Z","shell.execute_reply.started":"2023-12-24T21:31:41.551538Z","shell.execute_reply":"2023-12-24T21:31:49.322739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Word Cloud based on parties ----\n\nplt.figure(figsize = (20, 8))\n\n# WordCloud for the top 3 Political Parties\ncolumn = df_train[df_train['Party'].isin(['SYRIZA', 'ND', 'PASOK'])]['Text']\ncloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(' '.join(column))\n\nplt.imshow(cloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 3 Parties Word Cloud')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:31:53.921225Z","iopub.execute_input":"2023-12-24T21:31:53.921926Z","iopub.status.idle":"2023-12-24T21:31:58.960230Z","shell.execute_reply.started":"2023-12-24T21:31:53.921894Z","shell.execute_reply":"2023-12-24T21:31:58.959178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization, Word Embeddings & Data partioning","metadata":{}},{"cell_type":"code","source":"# ---- Embeddings, Train / Valid (test) ----\n\ndef word_embeddings(column, model):\n    embedding_vector = np.zeros((len(column), model.vector_size)) # Initialization of embeddings with zeros\n    word_count = [len(sublist) for sublist in column]             # Count the words of every text\n\n    for i, sentence in enumerate(column):\n        embeddings = [model.wv[word] for word in sentence if word in model.wv] # .wv is vector that describes the word in numbers\n        if word_count[i] > 0:\n            embedding_vector[i] = sum(embeddings) / word_count[i]\n            \n    return embedding_vector\n\nvector = Word2Vec(df_train['Tokens'], vector_size = 200, seed = 42, epochs = 20)\nvector.train(df_train['Tokens'], total_examples = len(df_train['Tokens']), epochs = vector.epochs)\n\nX_train = word_embeddings(df_train['Tokens'], vector)\nY_train = df_train['Sentiment_encoded']\n        \nX_valid = word_embeddings(df_valid['Tokens'], vector)\nY_valid = df_valid['Sentiment_encoded']","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:36:05.517821Z","iopub.execute_input":"2023-12-24T21:36:05.518236Z","iopub.status.idle":"2023-12-24T21:36:23.187640Z","shell.execute_reply.started":"2023-12-24T21:36:05.518204Z","shell.execute_reply":"2023-12-24T21:36:23.186645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensors","metadata":{}},{"cell_type":"code","source":"# ---- Saving to tensors ----\n\nx_train = torch.tensor(X_train, dtype = torch.float)      \ny_train = torch.tensor(Y_train.values, dtype = torch.long)\nx_valid = torch.tensor(X_valid, dtype = torch.float)\ny_valid = torch.tensor(Y_valid.values, dtype = torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:36:42.918974Z","iopub.execute_input":"2023-12-24T21:36:42.919369Z","iopub.status.idle":"2023-12-24T21:36:42.983105Z","shell.execute_reply.started":"2023-12-24T21:36:42.919329Z","shell.execute_reply":"2023-12-24T21:36:42.982222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"# ---- Neural Network with 3 hidden layers ----\n\nclass Neural_Network(nn.Module):\n    def __init__(self, input_size, hidden_layer1, hidden_layer2, hidden_layer3, output_size):\n        super(Neural_Network, self).__init__()\n        \n        torch.manual_seed(42)\n        self.activation = nn.ReLU()                          # Other Activation functions | PReLU, ReLU, ELU, Sigmoid, tanh\n        self.layer1 = nn.Linear(input_size, hidden_layer1)       \n        self.layer2 = nn.Linear(hidden_layer1, hidden_layer2)   \n        self.layer3 = nn.Linear(hidden_layer2, hidden_layer3)\n        self.output = nn.Linear(hidden_layer3, output_size)\n        \n    def forward(self, x):\n        layer1 = self.activation(self.layer1(x))\n        layer2 = self.activation(self.layer2(layer1))\n        layer3 = self.activation(self.layer3(layer2))\n        return self.output(layer3)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:36:50.424114Z","iopub.execute_input":"2023-12-24T21:36:50.425027Z","iopub.status.idle":"2023-12-24T21:36:50.431555Z","shell.execute_reply.started":"2023-12-24T21:36:50.424988Z","shell.execute_reply":"2023-12-24T21:36:50.430550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Setting up criterion, optimizer, data ----\n\nmodel = Neural_Network(x_train.shape[1], 128, 64, 32, 3)\n\ncriterion = nn.CrossEntropyLoss()                            # Initialize criterion\noptimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5) # Initialize optimizer\n\ndataset = TensorDataset(x_train, y_train)                           # Class to represent the data as list of tensors\ndataloader = DataLoader(dataset, batch_size = 64, shuffle = True)   # Initialize dataloader","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:36:58.267522Z","iopub.execute_input":"2023-12-24T21:36:58.267899Z","iopub.status.idle":"2023-12-24T21:36:58.288438Z","shell.execute_reply.started":"2023-12-24T21:36:58.267871Z","shell.execute_reply":"2023-12-24T21:36:58.287457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Training","metadata":{}},{"cell_type":"code","source":"# ---- Using forward & back propagation for training ----\n\nfor epoch in range(10):\n    losses = []\n    model.train()\n    for x, y in dataloader:\n        logits = model(x)           # Perform forward propagation\n        \n        loss = criterion(logits, y) # Calculating the loss using criterion function\n        losses.append(loss.item())\n        \n        optimizer.zero_grad()       # Delete previously stored gradients\n        loss.backward()             # Perform back propagation starting from the loss calculated in this epoch\n        optimizer.step()            # Update model's weights based on the gradients calculated during backprop\n        \n    print(f'Epoch {epoch}: Loss = {sum(losses) / len(dataloader):.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:38:02.971893Z","iopub.execute_input":"2023-12-24T21:38:02.972327Z","iopub.status.idle":"2023-12-24T21:38:15.681245Z","shell.execute_reply.started":"2023-12-24T21:38:02.972279Z","shell.execute_reply":"2023-12-24T21:38:15.680205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    logits_valid = model(x_valid)\n    probabilities = nn.functional.softmax(logits_valid, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\naccuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\nf1 = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)                 \nrecall = recall_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)        \nprecision = precision_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n                          \ntable = [['Method', 'Accuracy', 'F1-Score', 'Recall', 'Precision'],\n         ['Predict', accuracy, f1, recall, precision]]\n\nprint(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T21:38:37.687147Z","iopub.execute_input":"2023-12-24T21:38:37.688223Z","iopub.status.idle":"2023-12-24T21:38:37.721170Z","shell.execute_reply.started":"2023-12-24T21:38:37.688172Z","shell.execute_reply":"2023-12-24T21:38:37.720468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"# ---- Using Optuna ----\n\ndef objective(trial):\n    H1 = trial.suggest_int('H1', 64, 128)\n    H2 = trial.suggest_int('H2', 32, 64)\n    H3 = trial.suggest_int('H3', 8, 32)\n    model = Neural_Network(x_train.shape[1], H1, H2, H3, 3)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log = True)\n    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'AdamW'])\n    if optimizer == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n    elif optimizer == 'Adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n    elif optimizer == 'AdamW':\n        optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\n    dataset = TensorDataset(x_train, y_train)\n    dataloader = DataLoader(dataset, batch_size = 64, shuffle = True) \n    \n    for epoch in range(10):\n        model.train()\n        for x, y in dataloader:\n            logits = model(x)           # Perform forward propagation\n\n            loss = criterion(logits, y) # Calculating the loss using criterion function\n            \n            optimizer.zero_grad()       # Delete previously stored gradients\n            loss.backward()             # Perform back propagation starting from the loss calculated in this epoch\n            optimizer.step()            # Update model's weights based on the gradients calculated during backprop\n                    \n    model.eval()\n    with torch.no_grad():\n        logits_valid = model(x_valid)\n        probabilities = nn.functional.softmax(logits_valid, dim = 1)\n        predictions = torch.argmax(probabilities, dim = 1)\n\n    accuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\n        \n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize', study_name = 'Christos study')\nstudy.optimize(objective, n_trials = 100)\nbest_parameters = study.best_params\n\nbest_H1 = best_parameters['H1']\nbest_H2 = best_parameters['H2']\nbest_H3 = best_parameters['H3']\nbest_lr = best_parameters['learning_rate']\nbest_optimizer = best_parameters['optimizer']\n\nprint('Best hyperparameters: ', best_parameters)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:20:12.225322Z","iopub.execute_input":"2023-12-24T22:20:12.225702Z","iopub.status.idle":"2023-12-24T22:39:16.226357Z","shell.execute_reply.started":"2023-12-24T22:20:12.225673Z","shell.execute_reply":"2023-12-24T22:39:16.225570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network using Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"# ---- Setting up criterion, optimizer, data ----\n\nmodel = Neural_Network(x_train.shape[1], best_H1, best_H2, best_H3, 3)\n\ncriterion = nn.CrossEntropyLoss()\n\nif best_optimizer == 'SGD':\n    optimizer = torch.optim.SGD(model.parameters(), lr = best_lr)\nelif best_optimizer == 'Adam':\n    optimizer = torch.optim.Adam(model.parameters(), lr = best_lr)\nelif best_optimizer == 'AdamW':\n    optimizer = torch.optim.AdamW(model.parameters(), lr = best_lr)\n\ndataset = TensorDataset(x_train, y_train)                           # Class to represent the data as list of tensors\ndataloader = DataLoader(dataset, batch_size = 64, shuffle = True)   # Initialize dataloader","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:45:01.172567Z","iopub.execute_input":"2023-12-24T22:45:01.173002Z","iopub.status.idle":"2023-12-24T22:45:01.182692Z","shell.execute_reply.started":"2023-12-24T22:45:01.172965Z","shell.execute_reply":"2023-12-24T22:45:01.181748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training using Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"for epoch in range(10):\n    losses = []\n    model.train()\n    for x, y in dataloader:\n        logits = model(x)           # Perform forward propagation\n        \n        loss = criterion(logits, y) # Calculating the loss using criterion function\n        losses.append(loss.item())\n        \n        optimizer.zero_grad()       # Delete previously stored gradients\n        loss.backward()             # Perform back propagation starting from the loss calculated in this epoch\n        optimizer.step()            # Update model's weights based on the gradients calculated during backprop\n        \n    print(f'Epoch {epoch}: Loss = {sum(losses) / len(dataloader):.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:45:04.261109Z","iopub.execute_input":"2023-12-24T22:45:04.262163Z","iopub.status.idle":"2023-12-24T22:45:15.673266Z","shell.execute_reply.started":"2023-12-24T22:45:04.262113Z","shell.execute_reply":"2023-12-24T22:45:15.672407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation with Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    logits_valid = model(x_valid)\n    probabilities = nn.functional.softmax(logits_valid, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\naccuracy = accuracy_score(y_valid.numpy(), predictions.numpy())\nf1 = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)                 \nrecall = recall_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)        \nprecision = precision_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n                          \ntable = [['Method', 'Accuracy', 'F1-Score', 'Recall', 'Precision'],\n         ['Predict', accuracy, f1, recall, precision]]\n\nprint(tabulate(table, headers = 'firstrow', tablefmt = 'fancy_grid'))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:45:45.098784Z","iopub.execute_input":"2023-12-24T22:45:45.099781Z","iopub.status.idle":"2023-12-24T22:45:45.125647Z","shell.execute_reply.started":"2023-12-24T22:45:45.099742Z","shell.execute_reply":"2023-12-24T22:45:45.124570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Curve, ROC, Confusion Matrix","metadata":{}},{"cell_type":"code","source":"model = Neural_Network(x_train.shape[1], 128, 64, 32, 3)\n    \ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n\ndataset_train = TensorDataset(x_train, y_train)                               # Class to represent the data as list of tensors\ndataloader_train = DataLoader(dataset_train, batch_size = 64, shuffle = True) # Initialize dataloader\n\ndataset_valid = TensorDataset(x_valid, y_valid)                               # Class to represent the data as list of tensors\ndataloader_valid = DataLoader(dataset_valid, batch_size = 64, shuffle = True) # Initialize dataloader\n\nscore_train = []\nscore_valid = []\nlosses_train = []\nlosses_valid = []\n\nfor i in range(2):\n    if i == 0:\n        for epoch in range(10):\n            model.train()\n            for x, y in dataloader_train:\n                logits = model(x)           # Perform forward propagation\n\n                loss = criterion(logits, y) # Calculating the loss using criterion function\n\n                optimizer.zero_grad()       # Delete previously stored gradients\n                loss.backward()             # Perform back propagation starting from the loss calculated in this epoch\n                optimizer.step()            # Update model's weights based on the gradients calculated during backprop\n\n            model.eval()\n            with torch.no_grad():\n                logits_train = model(x_train)\n                probabilities = nn.functional.softmax(logits_train, dim = 1)\n                predictions = torch.argmax(probabilities, dim = 1)\n\n            f1_train = f1_score(y_train.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n            score_train.append(f1_train)\n\n            model.eval()\n            with torch.no_grad():\n                logits_valid = model(x_valid)\n                probabilities = nn.functional.softmax(logits_valid, dim = 1)\n                predictions = torch.argmax(probabilities, dim = 1)\n\n            f1_valid = f1_score(y_valid.numpy(), predictions.numpy(), average = 'macro', zero_division = 1)\n            score_valid.append(f1_valid)\n        \n    else:\n        for epoch in range(10):\n            losses = []\n            model.train()\n            for x, y in dataloader_train:\n                logits = model(x)           # Perform forward propagation\n\n                loss = criterion(logits, y) # Calculating the loss using criterion function\n                losses.append(loss.item())\n\n                optimizer.zero_grad()       # Delete previously stored gradients\n                loss.backward()             # Perform back propagation starting from the loss calculated in this epoch\n                optimizer.step()            # Update model's weights based on the gradients calculated during backprop\n\n            loss = sum(losses) / len(dataloader_train)\n            losses_train.append(loss)\n\n            losses = []\n            model.eval()\n            with torch.no_grad():\n                for x, y in dataloader_valid:\n                    logits = model(x)           # Perform forward propagation\n\n                    loss = criterion(logits, y) # Calculating the loss using criterion function\n                    losses.append(loss.item())\n\n                loss = sum(losses) / len(dataloader_valid)\n                losses_valid.append(loss)\n\nplt.figure(figsize = (20, 8))\n\nplt.subplot(1, 2, 1)\nplt.plot(score_train, label = 'Training')\nplt.plot(score_valid, label = 'Validation')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(losses_train, label = 'Training')\nplt.plot(losses_valid, label = 'Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:52:44.979666Z","iopub.execute_input":"2023-12-24T22:52:44.980175Z","iopub.status.idle":"2023-12-24T22:53:11.874822Z","shell.execute_reply.started":"2023-12-24T22:52:44.980141Z","shell.execute_reply":"2023-12-24T22:53:11.873719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- ROC Curve / AUC Score ----\n\nauc = np.round(roc_auc_score(y_valid.numpy(), probabilities.detach().numpy(), multi_class = 'ovo'), 3)\nprint('Total AUC: ', auc)\n\nscikitplot.metrics.plot_roc(y_valid.numpy(), probabilities.detach().numpy())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:47:29.406857Z","iopub.execute_input":"2023-12-24T22:47:29.407279Z","iopub.status.idle":"2023-12-24T22:47:29.691229Z","shell.execute_reply.started":"2023-12-24T22:47:29.407245Z","shell.execute_reply":"2023-12-24T22:47:29.690136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---- Confusion Matrix ----\n\nconfusion_martix = metrics.confusion_matrix(y_valid.numpy(), predictions.numpy())\n\nsns.heatmap(confusion_martix, annot = True, fmt = 'd', cmap = 'coolwarm')\nplt.ylabel('Prediction', fontsize = 13)\nplt.xlabel('Actual', fontsize = 13)\nplt.title('Confusion Matrix', fontsize = 17)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:47:33.221721Z","iopub.execute_input":"2023-12-24T22:47:33.222108Z","iopub.status.idle":"2023-12-24T22:47:33.463742Z","shell.execute_reply.started":"2023-12-24T22:47:33.222074Z","shell.execute_reply":"2023-12-24T22:47:33.462645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Predictions","metadata":{}},{"cell_type":"code","source":"def replace_sentiment(column):\n    map_sentiments = {\n        0: 'NEGATIVE',\n        1: 'NEUTRAL',\n        2: 'POSITIVE',\n    }\n    return list(map(map_sentiments.get, column))\n\nX_test = word_embeddings(df_test['Tokens'], vector)\nx_test = torch.tensor(X_test, dtype = torch.float)\n\nmodel.eval()\nwith torch.no_grad():\n    logits_test = model(x_test)\n    probabilities = nn.functional.softmax(logits_test, dim = 1)\n    predictions = torch.argmax(probabilities, dim = 1)\n\ndf_test['Sentiment'] = predictions\ndf_test['Sentiment'] = replace_sentiment(df_test['Sentiment'])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:47:37.449461Z","iopub.execute_input":"2023-12-24T22:47:37.449861Z","iopub.status.idle":"2023-12-24T22:47:37.876948Z","shell.execute_reply.started":"2023-12-24T22:47:37.449832Z","shell.execute_reply":"2023-12-24T22:47:37.876136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = df_test[['New_ID', 'Sentiment']]                                            # The dataset to output needs only\ndf_predictions = df_predictions.rename(columns = {'New_ID': 'Id', 'Sentiment': 'Predicted'}) # the 2 columns of sentiments and ids\ndf_predictions.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:47:40.145599Z","iopub.execute_input":"2023-12-24T22:47:40.146192Z","iopub.status.idle":"2023-12-24T22:47:40.168975Z","shell.execute_reply.started":"2023-12-24T22:47:40.146163Z","shell.execute_reply":"2023-12-24T22:47:40.168045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}